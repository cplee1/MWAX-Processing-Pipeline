#!/usr/bin/env nextflow

// Default params
params {
    // Software versions
    vcsbeam_version     = 'v4.2-21-ged32a7d'
    hyperdrive_version  = 'v0.3.0'
    birli_version       = 'v0.10.0'
    rclone_version      = '1.62.2'
}

// Singularity setup
process.module = 'singularity/3.7.4'
singularity {
    enabled = true
    // runOptions = '--nv -B /nvmetmp'
    runOptions = '-B /scratch'
    envWhitelist = 'SINGULARITY_BINDPATH, SINGULARITYENV_LD_LIBRARY_PATH'
}

// Job execution options
executor {
    $slurm {
        queueSize = 30  // Number of tasks handled in parallel
        submitRateLimit = '10 sec'  // Number of jobs submitted per second
        pollInterval = '30 sec'  // How often to poll the job status
        jobName = { "${task.process}_(${task.index})" }
    }
    $local {
        // Local jobs should not be resource intensive
        cpus = 4
        memory = 16.GB
    }
}

process {
    cache = 'lenient'
    shell = ['/bin/bash', '-euo', 'pipefail']

    withLabel: 'gpu|cpu' {
        executor = 'slurm'
        // By default use about 1/6 of a node
        cpus = 6
        memory = 60.GB
    }
    withLabel: gpu {
        queue   = 'gpuq'
    }
    withLabel: cpu {
        queue   = 'workq'
    }
    withLabel: python {
        beforeScript = "module use ${params.module_dir}; module load python/3.8.2"
    }
    withLabel: giant_squid {
        container = 'docker://mwatelescope/giant-squid:latest'
        containerOptions = '-B /scratch'
    }
    withLabel: birli {
        cpus = 36
        memory = 370.GB
        scratch = '/nvmetmp'
        beforeScript = "module use ${params.module_dir}; module load birli/${params.birli_version}"
        clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --tmp=440G --export=NONE" }
    }
    withLabel: hyperdrive {
        scratch = 'ram-disk'
        beforeScript = "module use ${params.module_dir}; module load hyperdrive/${params.hyperdrive_version}"
        clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --gres=tmp:50g,gpu:1 --export=NONE" }
    }
    withLabel: srclist {
        scratch = 'ram-disk'
        beforeScript = "module use ${params.module_dir}; module load hyperdrive/${params.hyperdrive_version}"
        clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --export=NONE" }
    }
    withLabel: vcsbeam {
        beforeScript = "module use ${params.module_dir}; module load vcsbeam/${params.vcsbeam_version}"
        clusterOptions = { "--ntasks=${params.num_chan} --ntasks-per-node=1 --cpus-per-task=1 --gpus-per-task=1 --gres=gpu:1 --export=NONE" }
    }
    withLabel: psranalysis {
        container = "file:///${params.container_dir}/psr-analysis/psr-analysis.sif"
    }
    withLabel: psrsearch {
        container = "file:///${params.container_dir}/psr-search/psr-search.sif"
    }
    withLabel: dspsr {
        clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --ntasks-per-node=1 --export=NONE" }
    }
    withLabel: prepfold {
        clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --ntasks-per-node=1 --export=NONE" }
    }
    withLabel: tar {
        cpus = 1
        clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --ntasks-per-node=1 --export=NONE" }
    }
    withLabel: copy {
        module = 'singularity/3.11.4-slurm'
        executor = 'slurm'
        queue = 'copy'
        beforeScript = "module load rclone/${params.rclone_version}"
        clusterOptions = { "--account=mwavcs --clusters=setonix --ntasks=1 --cpus-per-task=4 --mem-per-cpu=8G --export=NONE" }
    }
}
