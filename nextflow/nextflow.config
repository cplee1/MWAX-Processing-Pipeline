#!/usr/bin/env nextflow

// =============================================================================
// GLOBAL CONFIG
// =============================================================================

params {
    help = false
    
    // mwax_download.nf ........................................................
    skip_download = false  // Skip downloading the data
    calids        = null   // obs IDs of calibrators
    asvo_id_obs   = null   // ASVO ID for VCS observation
    asvo_id_cals  = null   // ASCO ID for calibrator observations
    offset        = null     // Time offset from beginning of observation
    asvo_api_key  = null     // ASVO API key for recognising user
    giant_squid   = '/opt/cargo/bin/giant-squid'  // Path within docker image

    // mwax_calibrate.nf .......................................................
    // General options
    cal_joblist      = null  // list of calibration jobs
    obsid            = null  // ObsID of VCS observation to calibrate
    calibrators      = null  // Comma separated list of CALID:SOURCE
    // Birli options
    force_birli      = false  // Force Birli to run
    skip_birli       = false  // Force Birli not to run
    df               = 40  // Desired freq. resolution in kHz
    dt               = 2   // Desired time resolution in seconds
    // Hyperdrive options
    flagged_tiles      = ''           // Space separated list of tiles to flag
    flagged_fine_chans = '0 1 30 31'  // Space separated list of fine channels to flag per coarse channel

    // mwax_beamform.nf ........................................................
    // General options
    skip_bf           = false  // Skip beamforming and just fold
    // VCSBeam options
    fits              = false  // Export PSRFITS data
    vdif              = false  // Export VDIF data
    low_chan          = 109    // Lowest coarse channel
    num_chan          = 24     // Number of coarse channels
    duration          = null   // Observation length
    begin             = null   // Start GPS time
    calid             = null   // ObsID of calibration observation
    psrs              = null   // List of pulsars to beamform on
    pointings         = null   // List of pointings to beamform
    pointings_file    = null   // File with pointings to beamform on
    convert_rts_flags = false  // Convert tile indices to TileNames
    // Folding and dedispersion options
    nbin        = 128  // Maximum bins to fold into (dspsr & prepfold)
    fine_chan   = 32   // Number of fine channels per coarse channel (dspsr)
    tint        = 8    // Sub-integration time in seconds (dspsr)
    force_psrcat = false
    // Search options
    nosearch_prepfold   = false  // Do not run prepfold search
    nosearch_pdmp       = false  // Do not run pdmp search
    nsub                = 192    // Number of sub-bands to use in prepfold search
    npart               = 256    // Number of sub-integrations to use in prepfold search
    pdmp_mc             = 192    // Maximum number of frequency channels to use in pdmp search
    pdmp_ms             = 256    // Maximum number of sub-integrations to use in pdmp search
    // Acacia upload options
    acacia_profile      = 'mwavcs'
    acacia_bucket       = 'smart'
    acacia_prefix_base  = null  // Will save to ${acacia_prefix_base}/${obsid}
}

// =============================================================================
//   CLUSTER SPECIFIC CONFIG
// =============================================================================
def hostname = 'hostname'.execute().text.trim().replace("-", "")

// GARRAWARLA ------------------------------------------------------------------
if ( hostname.startsWith('garrawarla') ) {
    // PIPELINE DEFAULTS .......................................................
    
    // Directory where intermediate pipeline products will be written
    workDir = "/astro/mwavcs/$USER/mwax_processing_work"

    params {
        // Software versions
        vcsbeam_version     = 'v4.2-18-gdf6a71a'
        hyperdrive_version  = 'v0.3.0'
        birli_version       = 'v0.10.0'
        rclone_version      = '1.62.2'
        // Directory containing downloaded VCS data organised by obs IDs
        vcs_dir = "/astro/mwavcs/$USER"
        // Directory where ASVO places downloaded data
        asvo_dir = '/astro/mwavcs/asvo'
        // Location of required modules
        module_dir = '/pawsey/mwa/software/python3/modulefiles'
        // Location of singularity
        container_dir = '/pawsey/mwa/singularity'
        // Directory containing pulsar ephemerides (takes preference over PSRCAT)
        ephemeris_dir = '/astro/mwavcs/cplee/remote_backup/meerkat_ephemerides'
        // Path to jq binary
        jq = '/astro/mwavcs/cplee/bin/jq'
        // Directory with specific source models
        models_dir = '/pawsey/mwa/software/python3/mwa-reduce/mwa-reduce-git/models'
        // Path to source catalogue
        src_catalogue = '/astro/mwavcs/cplee/remote_backup/source_lists/GGSM_updated.txt'
        // Path to conversion script
        convert_flags_script = '/astro/mwavcs/cplee/github/vcsbeam/utils/rts_flag_ant_to_tilenames.py'
    }

    // RESOURCES ...............................................................
    process.module = 'singularity/3.7.4'
    singularity {
        enabled = true
        // runOptions = '--nv -B /nvmetmp'
        envWhitelist = 'SINGULARITY_BINDPATH, SINGULARITYENV_LD_LIBRARY_PATH'
    }

    executor {
        $slurm {
            queueSize = 30  // Number of tasks handled in parallel
            submitRateLimit = '10 sec'  // Number of jobs submitted per second
            pollInterval = '30 sec'  // How often to poll the job status
            jobName = { "${task.process}_(${task.index})" }
        }
        $local {
            // Local jobs should not be resource intensive
            cpus = 4
            memory = 16.GB
        }
    }

    process {
        cache = 'lenient'

        withLabel: 'gpu|cpu' {
            executor = 'slurm'
            // By default use about 1/6 of a node
            cpus = 6
            memory = 60.GB
        }
        withLabel: gpu {
            queue   = 'gpuq'
        }
        withLabel: cpu {
            queue   = 'workq'
        }
        withLabel: giant_squid {
            container = 'docker://mwatelescope/giant-squid:latest'
            containerOptions = '--bind /astro'
        }
        withLabel: birli {
            cpus = 36
            memory = 370.GB
            scratch = '/nvmetmp'
            beforeScript = "module use ${params.module_dir}; module load birli/${params.birli_version}"
            clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --tmp=440G --export=NONE" }
        }
        withLabel: hyperdrive {
            scratch = 'ram-disk'
            beforeScript = "module use ${params.module_dir}; module load hyperdrive/${params.hyperdrive_version}"
            clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --gres=tmp:50g,gpu:1 --export=NONE" }
        }
        withLabel: srclist {
            scratch = 'ram-disk'
            beforeScript = "module use ${params.module_dir}; module load hyperdrive/${params.hyperdrive_version}"
            clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --export=NONE" }
        }
        withLabel: vcsbeam {
            beforeScript = "module use ${params.module_dir}; module load vcsbeam/${params.vcsbeam_version}"
            clusterOptions = { "--ntasks=${params.num_chan} --ntasks-per-node=1 --cpus-per-task=1 --gpus-per-task=1 --gres=gpu:1 --export=NONE" }
        }
        withLabel: psranalysis {
            container = "file:///${params.container_dir}/psr-analysis/psr-analysis.sif"
        }
        withLabel: psrsearch {
            container = "file:///${params.container_dir}/psr-search/psr-search.sif"
        }
        withLabel: dspsr {
            clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --ntasks-per-node=1 --export=NONE" }
        }
        withLabel: prepfold {
            clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --ntasks-per-node=1 --export=NONE" }
        }
        withLabel: tar {
            cpus = 1
            clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --ntasks-per-node=1 --export=NONE" }
        }
        withLabel: copy {
            module = 'singularity/3.11.4-slurm'
            executor = 'slurm'
            queue = 'copy'
            beforeScript = "module load rclone/${params.rclone_version}"
            clusterOptions = { "--account=mwavcs --clusters=setonix --ntasks=1 --cpus-per-task=4 --mem-per-cpu=8G --export=NONE" }
        }
    }
}

includeConfig './cplee_custom.config'
